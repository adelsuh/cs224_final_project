{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS6ESNtSpN-z"
   },
   "source": [
    "# CS224W Final Project: Tutorial on the Augmentation of Graphs in PyG\n",
    "\n",
    "### Jerry Chan, Jihee Suh, John So"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvHgPTT8qnlm"
   },
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPsmdGx7UJ4z"
   },
   "source": [
    "### Install PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "pS2qTpqApIRS",
    "outputId": "00dfed58-8148-4f6c-ef0a-45d6ea01cee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch_version = str(torch.__version__)\n",
    "if \"2.4.0\" not in torch_version:\n",
    "  !pip install torch==2.4.0 -q\n",
    "print(torch_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "dies5CJuqmum",
    "outputId": "d47159ad-84ad-4602-d944-78965967cee1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_version' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scatter_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch-geometric.com/whl/torch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch_version\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m sparse_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch-geometric.com/whl/torch-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch-scatter -f $scatter_src -q\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch_version' is not defined"
     ]
    }
   ],
   "source": [
    "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "!pip install torch-scatter -f $scatter_src -q\n",
    "!pip install torch-sparse -f $sparse_src -q\n",
    "!pip install torch-geometric -q\n",
    "!pip install ogb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p_CLxHV2M3Wh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn.models import GraphSAGE\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_undirected\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0uIk066oMxUJ",
    "outputId": "51f627bb-fa8f-4908-bae3-75e36d220b20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euGXjAPU1Ss4"
   },
   "source": [
    "### Dataset and Tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HmhMpxvuxO0",
    "outputId": "986d7044-1dbd-4923-c96d-5ed8e7e07fc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerrychan/miniconda3/envs/graph/lib/python3.10/site-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=2449029, edge_index=[2, 123718280], x=[2449029, 100], y=[2449029, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = PygNodePropPredDataset(name='ogbn-products', root='./products/')\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "# sample test set to speed up\n",
    "split_idx['test'] = split_idx['test'][:10000]\n",
    "data = dataset[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRim-oBTUVSe"
   },
   "source": [
    "### Training and Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFLmCKpCdEEH",
    "outputId": "d1fc1e9d-efc4-4bc6-9922-05a1e76c76bc"
   },
   "outputs": [],
   "source": [
    "# Model settings\n",
    "input_dim = dataset[0].x.shape[1]\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "# Training settings\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 20\n",
    "\n",
    "# Dataloader settings\n",
    "batch_size = 32\n",
    "fan_out = 10\n",
    "dataloader_num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGENodeClassification(\n",
       "  (graph_sage): GraphSAGE(100, 128, num_layers=2)\n",
       "  (cls_head): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=47, bias=True)\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GraphSAGENodeClassification(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(GraphSAGENodeClassification, self).__init__()\n",
    "        self.graph_sage = GraphSAGE(in_channels = input_dim, hidden_channels = hidden_dim, num_layers=num_layers)\n",
    "        self.cls_head = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, num_classes),\n",
    "        )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.graph_sage(x, edge_index)\n",
    "        return self.cls_head(h)\n",
    "\n",
    "model = GraphSAGENodeClassification(input_dim, hidden_dim, num_layers, dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqeOx2Qjd5Br",
    "outputId": "c51c3f17-6396-4735-c342-d565b82919a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerrychan/miniconda3/envs/graph/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Data Loaders with 196615 training, 39323 validation, and 10000 test nodes.\n"
     ]
    }
   ],
   "source": [
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['train'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=dataloader_num_workers\n",
    ")\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['valid'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=dataloader_num_workers,\n",
    ")\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['test'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Initialized Data Loaders with {len(split_idx['train'])} training, {len(split_idx['valid'])} validation, and {len(split_idx['test'])} test nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "KWZejP8NNFlG"
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_one_epoch(model, dataloader, optimizer, transform=None):\n",
    "    model.train()\n",
    "\n",
    "    # define states\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if transform is not None:\n",
    "          batch = transform(batch)\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch.x, batch.edge_index)[:batch_size]\n",
    "\n",
    "        # backward pass\n",
    "        labels = batch.y[:batch_size].squeeze(-1)\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log stats\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        num_examples += batch_size\n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc\n",
    "\n",
    "# test process\n",
    "@torch.no_grad()\n",
    "def test(model, dataloader, transform=None, apply_transform=True):\n",
    "    model.eval()\n",
    "\n",
    "    # define states\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if apply_transform and (transform is not None):\n",
    "          batch = transform(batch)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(batch.x, batch.edge_index)[:batch_size]\n",
    "        labels = batch.y[:batch_size].squeeze(-1)\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        \n",
    "        # log stats\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        num_examples += batch_size\n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813
    },
    "id": "e41neWDGY_Mw",
    "outputId": "e68ebae0-d8db-48b8-f8fc-1e74a8e4a046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 788/6145 [00:07<00:49, 107.63it/s]"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f'Epoch: {epoch:02d}')\n",
    "\n",
    "    # training\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer)\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {100.0 * train_acc:.2f}%')\n",
    "\n",
    "    # validation\n",
    "    val_loss, val_acc = test(model, val_loader)\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Accuracy: {100.0 * val_acc:.2f}%')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print('New best validation accuracy, saving model...')\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "\n",
    "print(f'Best Validation Accuracy: {100.0 * best_val_acc:.2f}%')\n",
    "\n",
    "# eval best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_final_acc = test(model, train_loader)\n",
    "print(f'Test Accuracy: {100.0 * test_final_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFw4Vi5o1dPP"
   },
   "source": [
    "## Training Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twbc7Fk8EogI"
   },
   "source": [
    "Half-Hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLvQwDATEppX"
   },
   "outputs": [],
   "source": [
    "halfhop = T.HalfHop(alpha=0.5, p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIEUAN-bG1Bg"
   },
   "outputs": [],
   "source": [
    "def train_halfhop(model, optimizer, dataloader: NeighborLoader, transform=None) -> tuple[torch.Tensor, float]:\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        if transform is not None:\n",
    "          batch = transform(batch)\n",
    "        out = model(batch.x, batch.edge_index)[~batch.slow_node_mask][:batch.batch_size]\n",
    "        y = batch.y[:batch.batch_size].squeeze().to(torch.long)\n",
    "        loss = torch.nn.functional.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y).sum())\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / split_idx['train'].size(0)\n",
    "    return loss, approx_acc\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_halfhop(model, dataloader: NeighborLoader, transform=None) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = total_examples = 0\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        if transform is not None:\n",
    "          batch = transform(batch)\n",
    "        out = model(batch.x, batch.edge_index)[~batch.slow_node_mask]\n",
    "        pred = out.argmax(dim=-1)\n",
    "        y = batch.y.view(-1).to(torch.long)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(out, y)\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        total_correct += int((pred == y).sum())\n",
    "        total_examples += y.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), total_correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOt37j8DFJk3",
    "outputId": "2389d6de-d799-4cd6-8f9f-2eaf283dd592"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2842/2842 [00:41<00:00, 68.16it/s]\n",
      "100%|██████████| 932/932 [00:10<00:00, 86.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train Loss: 2.5389, Train Acc: 34.33%\n",
      "Val Loss: 2.1922, Val Acc: 39.85%\n",
      "Best Validation Accuracy: 39.85%\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1519/1519 [00:06<00:00, 247.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 37.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.reset_parameters()\n",
    "\n",
    "times = []\n",
    "best_val = 0.\n",
    "num_epochs = 1\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_halfhop(model, optimizer, train_loader, transform=halfhop)\n",
    "    val_loss, val_acc = test_halfhop(model, val_loader, transform=halfhop)\n",
    "\n",
    "    print(f'Epoch {epoch:02d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc * 100.0:.2f}%',)\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100.0:.2f}%')\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "\n",
    "print(f'Best Validation Accuracy: {100.0 * best_val:.2f}%')\n",
    "\n",
    "print('Testing...')\n",
    "test_loss, test_final_acc = test_halfhop(model, test_loader, transform=halfhop)\n",
    "print(f'Test Accuracy: {100.0 * test_final_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYXn7Cqm-Ff1"
   },
   "source": [
    "### Mask Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS9gFFgs1sko"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import mask_feature\n",
    "\n",
    "def train_with_mask(model, optimizer, dataloader: NeighborLoader, p = 0.2) -> tuple[torch.Tensor, float]:\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_correct = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        masked_x, feature_mask = mask_feature(batch.x, p)\n",
    "        out = model(masked_x, batch.edge_index)[:batch.batch_size]\n",
    "        y = batch.y[:batch.batch_size].squeeze().to(torch.long)\n",
    "        loss = torch.nn.functional.cross_entropy(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        total_correct += int(out.argmax(dim=-1).eq(y).sum())\n",
    "    loss = total_loss / len(train_loader)\n",
    "    approx_acc = total_correct / split_idx['train'].size(0)\n",
    "    return loss, approx_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZZXjwEL94H5"
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "best_val = 0.\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = train_with_mask(model, optimizer, train_loader)\n",
    "    val_loss, val_acc = test(model, val_loader)\n",
    "\n",
    "    print(f'Epoch {epoch:02d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc * 100.0:.2f}%',)\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100.0:.2f}%')\n",
    "\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "\n",
    "print(f'Best Validation Accuracy: {100.0 * best_val:.2f}%')\n",
    "\n",
    "print('Testing...')\n",
    "test_loss, test_final_acc = test(test_loader)\n",
    "print(f'Test Accuracy: {100.0 * test_final_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
