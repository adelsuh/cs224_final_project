{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adelsuh/cs224_final_project/blob/main/graph_structure_aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS224W Final Project: Tutorial on the Augmentation of Graphs in PyG using Structural Augmentations\n",
    "\n",
    "### Jerry Chan, Jihee Suh, John So"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a widely used technique that leverages existing data to further train a model, improving its performance and generalization. For structured data formats such as images, augmentation methods can be quite straightforward, including operations like cropping, resizing, rotating, and adding noise. These augmentations are useful for reducing overfitting to the training dataset and adding invariance to certain transformations, such as color shifts, different camera models, and even different camera poses.\n",
    "\n",
    "PyG provides support for dataset augmentations, which primarily inherit from the `torch_geometric.transforms` class. In this Colab, we will cover how augmentations can modify the graph structure to improve performance in inductive graph learning settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook setup: install PyG + torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pS2qTpqApIRS",
    "outputId": "f2eddb29-24ff-4935-b8a0-7836789c5a56"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_version = str(torch.__version__)\n",
    "if \"2.4.0\" not in torch_version:\n",
    "  !pip install torch==2.4.0 -q\n",
    "print(torch_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "dies5CJuqmum"
   },
   "outputs": [],
   "source": [
    "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "!pip install torch-scatter -f $scatter_src -q\n",
    "!pip install torch-sparse -f $sparse_src -q\n",
    "!pip install torch-geometric -q\n",
    "!pip install ogb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uIk066oMxUJ",
    "outputId": "e1e6d757-9329-4f50-dd82-e0ebb2562660"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the dataset and tasks\n",
    "\n",
    "The below code sets up some hyperparameters which will be used in dataloading and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg2APSF7WvSq",
    "outputId": "fd588efb-a80a-4dac-895b-3530d8c81346"
   },
   "outputs": [],
   "source": [
    "# Model settings\n",
    "hidden_dim = 128 #@param {type: \"integer\"}\n",
    "num_layers = 4 #@param {type: \"integer\"}\n",
    "# Training settings\n",
    "learning_rate = 0.0001 #@param {type: \"number\"}\n",
    "num_epochs = 25 #@param {type: \"integer\"}\n",
    "\n",
    "# Dataloader settings\n",
    "batch_size = 32 #@param {type: \"integer\"}\n",
    "fan_out = 30 #@param {type: \"integer\", hint: \"Used in neighborhood sampling to sample a subgraph\"}\n",
    "dataloader_num_workers = 2 #@param {type: \"integer\"}\n",
    "\n",
    "print(f\"\"\"\n",
    "Running training with the following configuration:\n",
    "   hidden_dim: {hidden_dim}\n",
    "   num_layers: {num_layers}\n",
    "   learning_rate: {learning_rate}\n",
    "   num_epochs: {num_epochs}\n",
    "   batch_size: {batch_size}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Node property prediction with GraphSAGE\n",
    "\n",
    "For this tutorial, we will consider the [ogbn-arxiv](https://ogb.stanford.edu/docs/nodeprop/) dataset from the Open Graph Benchmark (OGB). This dataset consists of ~170K nodes and ~1.2M directed edges. Each node represents an arXiv CS paper, and each edge represents the citation network between arXiv papers. The prediction task is to predict one of 40 labels for each node (i.e. CS.AI, CS.OS, etc.), given a 128-dimensional feature vector consisting of averaged language embeddings for the arXiv paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below block to create the dataset. If this is your first time loading the dataset, it will additionally prompt you to download files.\n",
    "\n",
    "**Note**: this block loads the dataset into RAM each time it is called! So calling this block multiple times will likely consume all of the notebook's RAM. Take caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjEjRXaMZLqZ",
    "outputId": "c6db83f1-00a8-4a28-9189-58789f0e01b6"
   },
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='./arxiv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll approach this as an *inductive graph prediction* problem; we want to train one network across many graphs, such that when given new nodes in the graph, or entirely new graphs, our predictor can generalize. To aid with this, the `ogbn-arxiv` dataset provides dataset splits:\n",
    "\n",
    "- **train split**: CS papers published up until 2017\n",
    "- **validation split**: CS papers published during 2018\n",
    "- **test split**: CS papers published during or after 2019.\n",
    "\n",
    "We'll train on the subgraphs on the train split, then optimize parameters based on performance on the validation split. Lastly, we'll report the performance of methods on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKdSKWebalo3",
    "outputId": "ec7c5d07-7c01-4114-f506-77404dd185f5"
   },
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split()\n",
    "# sample test set to speed up\n",
    "split_idx['test'] = split_idx['test']\n",
    "split_idx['valid'] = split_idx['valid']\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary of the OBGN Arxiv dataset:\n",
    "  Number of graphs: {len(dataset)}\n",
    "  Number of features: {dataset.num_features}\n",
    "  Number of classes: {dataset.num_classes}\n",
    "  Length of each split:\n",
    "    Training: {len(split_idx['train'])}\n",
    "    Validation: {len(split_idx['valid'])}\n",
    "    Test: {len(split_idx['test'])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some dataloaders. Training GNNs on graphs with 100k+ node is computationally prohibitive, making full-batch training infeasible. To address this, we can use neighbor sampling, a technique designed for efficient mini-batch training on large graphs by sampling smaller subgraphs. In particular, PyG provides native functionality for this with the `NeighborLoader` class.\n",
    "\n",
    "The `num_neighbors` parameter controls how neighbors are sampled. It consists of a length $k$ list of integers, and performs $k$ sampling iterations. Starting from a sampled node, NeighborLoader samples `num_neighbors[i]` neighbors from nodes involved in the previous iteration. In our code, we sample $k$-hop neighborhoods, where $k$ is the depth of our GNN. This approach ensures scalability by iteratively constructing smaller, representative sub-graphs while preserving the graph structure necessary for effective learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3loxjdsYkwv",
    "outputId": "9a81e1c7-3f4b-4a12-bfb3-9f3e9416705b"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "data = dataset[0]\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['train'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=dataloader_num_workers\n",
    ")\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['valid'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=dataloader_num_workers,\n",
    ")\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['test'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HmhMpxvuxO0",
    "outputId": "670b02fc-852c-4378-85ff-08a1bd071d52"
   },
   "outputs": [],
   "source": [
    "print(f\"Example batch:\")\n",
    "train_batch = next(iter(train_loader))\n",
    "print(train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, let's now choose a GNN model. For the purposes of this tutorial, we will choose a simple yet powerful network. [GraphSage](https://arxiv.org/abs/1706.02216) (Hamilton 2017) leverages node features and neighborhood aggregation to learn deep features for nodes. This lends itself to a variety of prediction problems, including our node classification problem.\n",
    "\n",
    "PyG includes a native implementation of [GraphSAGE](https://pytorch-geometric.readthedocs.io/en/2.5.3/generated/torch_geometric.nn.models.GraphSAGE.html), which we will use to construct our predictor. The predictor consists of a GraphSAGE model to learn deep node embeddings. Then, we pass in a small MLP to output prediction logits for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RGBPUxkQ2R7H"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GraphSAGE\n",
    "\n",
    "input_dim = dataset.num_features\n",
    "\n",
    "def get_model():\n",
    "    class GraphSAGENodeClassification(torch.nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "            super(GraphSAGENodeClassification, self).__init__()\n",
    "            self.graph_sage = GraphSAGE(in_channels = input_dim, hidden_channels = hidden_dim, num_layers=num_layers)\n",
    "            self.cls_head = torch.nn.Sequential(\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, num_classes),\n",
    "            )\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.graph_sage(x, edge_index)\n",
    "            return self.cls_head(h)\n",
    "\n",
    "    model = GraphSAGENodeClassification(input_dim, hidden_dim, num_layers, dataset.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define some helper functions, `train_one_epoch` and `test` below. Notably, our functions will take in two arguments:\n",
    "- `transform`: whether to apply a unary transformation function to the data, such as removing nodes or adding features.\n",
    "- `filter_output_fn`: whether to apply a binary transformation function to the output. This is intended to handle any extra information introduced by the transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KWZejP8NNFlG"
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_one_epoch(model,\n",
    "                    dataloader,\n",
    "                    optimizer,\n",
    "                    transform=None,\n",
    "                    filter_output_fn=None):\n",
    "    \"\"\"\n",
    "    Run one epoch of training on the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
    "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
    "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # define stats\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if transform is not None:\n",
    "          batch = transform(batch)\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch.x, batch.edge_index)\n",
    "        if filter_output_fn is not None:\n",
    "          logits = filter_output_fn(logits, batch)\n",
    "\n",
    "        # backward pass\n",
    "        num_labels = logits.shape[0]\n",
    "        labels = batch.y.squeeze(-1)\n",
    "\n",
    "        # select supervision nodes\n",
    "        labels = labels[:batch_size]\n",
    "        logits = logits[:batch_size]\n",
    "        num_examples += batch_size\n",
    "\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # log stats\n",
    "        total_loss += loss.item() * num_labels\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc\n",
    "\n",
    "# test process\n",
    "@torch.no_grad()\n",
    "def test(model,\n",
    "         dataloader,\n",
    "         transform=None,\n",
    "         filter_output_fn=None,\n",
    "         apply_transform=True):\n",
    "    \"\"\"\n",
    "    Calculate metrics for the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
    "        apply_transform: whether to use the arguments transform and filter_output_fn.\n",
    "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
    "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # define states\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if apply_transform and (transform is not None):\n",
    "          batch = transform(batch)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(batch.x, batch.edge_index)\n",
    "        if apply_transform and (filter_output_fn is not None):\n",
    "          logits = filter_output_fn(logits, batch)\n",
    "\n",
    "        # compute loss\n",
    "        num_labels = logits.shape[0]\n",
    "        labels = batch.y.squeeze(-1)\n",
    "\n",
    "        # select supervision nodes\n",
    "        labels = labels[:batch_size]\n",
    "        logits = logits[:batch_size]\n",
    "        num_examples += batch_size\n",
    "\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        # log stats\n",
    "        total_loss += loss.item() * num_labels\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        \n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and evaluate the model, call the below `train` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e41neWDGY_Mw"
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          num_epochs,\n",
    "          transform=None,\n",
    "          filter_output_fn=None,\n",
    "          apply_transform_at_test=True):\n",
    "    all_train_acc, all_val_acc, all_test_acc = [], [], []\n",
    "    best_val_ind, best_val_acc = 0, 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch: {epoch+1:02d}')\n",
    "\n",
    "        # training\n",
    "        train_loss, train_acc = train_one_epoch(model,\n",
    "                                                train_loader,\n",
    "                                                optimizer,\n",
    "                                                transform,\n",
    "                                                filter_output_fn)\n",
    "        val_loss, val_acc = test(model,\n",
    "                                 val_loader,\n",
    "                                 transform,\n",
    "                                 filter_output_fn=filter_output_fn,\n",
    "                                 apply_transform=apply_transform_at_test)\n",
    "        test_loss, test_acc = test(model,\n",
    "                                   test_loader,\n",
    "                                   transform,\n",
    "                                   filter_output_fn=filter_output_fn,\n",
    "                                   apply_transform=apply_transform_at_test)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_ind = epoch\n",
    "\n",
    "        print(f'Train {train_loss:.4f} ({100.0 * train_acc:.2f}%) | Val {val_loss:.4f} ({100.0 * val_acc:.2f}%) | Test {test_loss:.4f} ({100.0 * test_acc:.2f}%)')\n",
    "\n",
    "        all_train_acc.append(train_acc)\n",
    "        all_val_acc.append(val_acc)\n",
    "        all_test_acc.append(test_acc)\n",
    "\n",
    "    return {\n",
    "        'all_train_acc': np.array(all_train_acc),\n",
    "        'all_val_acc': np.array(all_val_acc),\n",
    "        'all_test_acc': np.array(all_test_acc),\n",
    "        'best_val_ind': best_val_ind,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7689hVEydrU7"
   },
   "source": [
    "To illustrate some example usage, let's run a baseline. This trains a GraphSAGE network with no graph structure augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJREZyVzPqIr",
    "outputId": "78657c28-589e-4515-e79a-7b1cc57b74bd"
   },
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "results = train(model, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "best_bl_train_acc = results['all_train_acc'][results['best_val_ind']]\n",
    "best_bl_val_acc = results['all_val_acc'][results['best_val_ind']]\n",
    "best_bl_test_acc = results['all_test_acc'][results['best_val_ind']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFw4Vi5o1dPP"
   },
   "source": [
    "## Training Augmentation\n",
    "\n",
    "Beyond augmentations on the graph and its nodes as covered above, it is useful to dynamically augment batches during training, analogous to random cropping, blurring, and color shifting for images. To accomplish this, we refer to several methods in torch_geometric.utils. In this section, we introduce two training augmentation methods: `mask_feature` and `dropout_edge` These methods modify the sampled graph during training to dynamically perturb the input, preventing overfitting by discouraging the model from over-relying on specific features or edges. This approach leads to a more robust and generalizable model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktRl8GWF2R7I"
   },
   "source": [
    "### Dropout Edge\n",
    "The `dropout_edge` function randomly removes edges from the graph. It operates on the edge_index matrix and returns:\n",
    "* A modified `edge_index` with some edges dropped.\n",
    "* A binary tensor `edge_mask` indicating which edges were retained (True) or dropped (False).\n",
    "\n",
    "Key Arguments:\n",
    "\n",
    "* `p`: The probability of dropping an edge.\n",
    "* `Force_undirected`: When set to True setting ensures that the resulting edge_index remains undirected.\n",
    "\n",
    "The following code applied dropout edge with `p` $\\in$ [0.1, 0.2, 0.3, 0.4, 0.5]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "def dropout_edge_batch(batch, p):\n",
    "    batch.edge_index, removed_edge = dropout_edge(batch.edge_index, p=p)\n",
    "    return batch\n",
    "\n",
    "rows = []\n",
    "for prob in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking probability of {prob}\\n\")\n",
    "    transform = partial(dropout_edge_batch, p=prob)\n",
    "    result = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=num_epochs)\n",
    "    rows.append({\n",
    "        \"edge_prob\":prob,\n",
    "        \"test_acc\": result['all_test_acc'][result['best_val_ind']],\n",
    "        \"val_acc\": result['all_val_acc'][result['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"out/dropout_edge_result.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('out/dropout_edge_result.csv', index_col=0)\n",
    "df.loc[5] = [0, 0.5418, 0.615]\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "ax = sns.lineplot(x='edge_prob', y='test_acc', data=df)\n",
    "\n",
    "ax.set(xlabel='Edge Dropout Probability', ylabel='Test Accuracy')\n",
    "ax.set_yticks([0.54, 0.55, 0.56], [\"54%\", \"55%\", \"56%\"])\n",
    "ax.set_ylim(0.538, 0.562)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Feature\n",
    "\n",
    "The `mask_feature` function randomly masks parts of node features. It takes the node feature matrix `x` as input and returns the modified features along with a mask indicating the positions of the masked features.\n",
    "\n",
    "Key arguments:\n",
    "* `p`: The probability of masking a feature.\n",
    "* `fill_value`: The value used to replace masked features (default: 0).\n",
    "* `mode`: The masking scheme.\n",
    "\n",
    "There are three masking modes:\n",
    "* `col` (default): Masks entire feature columns across all nodes.\n",
    "* `row`: Masks all features of selected nodes.\n",
    "* `all`: Masks individual features independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code applied feature masking with `p` $\\in$ [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import mask_feature\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "def mask_feature_batch(batch, p, mode=\"all\"):\n",
    "    masked_x, feature_mask = mask_feature(batch.x, p = p, mode = mode)\n",
    "    batch.x = masked_x\n",
    "    return batch\n",
    "\n",
    "rows = []\n",
    "for masking_prob in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking probability of {masking_prob}\\n\")\n",
    "    transform = partial(mask_feature_batch, p=masking_prob)\n",
    "    results = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=25)\n",
    "    rows.append({\n",
    "        \"masking_prob\": masking_prob,\n",
    "        \"test_acc\": results['all_test_acc'][results['best_val_ind']],\n",
    "        \"val_acc\": results['all_val_acc'][results['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"out/masking_prob_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code applied feature masking with `mode` $\\in$ [\"all\", \"col\", \"row\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for masking_mode in [\"all\", \"col\", \"row\"]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking mode {masking_mode}\\n\")\n",
    "    transform = partial(mask_feature_batch, p=0.05, mode=masking_mode)\n",
    "    results = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=25)\n",
    "    rows.append({\n",
    "        \"masking_mode\": masking_mode,\n",
    "        \"test_acc\": results['all_test_acc'][results['best_val_ind']],\n",
    "        \"val_acc\": results['all_val_acc'][results['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"out/masking_mode_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
