{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/adelsuh/cs224_final_project/blob/main/graph_structure_aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS6ESNtSpN-z"
   },
   "source": [
    "# CS224W Final Project: Tutorial on the Augmentation of Graphs in PyG\n",
    "\n",
    "### Jerry Chan, Jihee Suh, John So"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPsmdGx7UJ4z"
   },
   "source": [
    "### Notebook setup: install PyG + torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "pS2qTpqApIRS",
    "outputId": "f2eddb29-24ff-4935-b8a0-7836789c5a56"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_version = str(torch.__version__)\n",
    "if \"2.4.0\" not in torch_version:\n",
    "  !pip install torch==2.4.0 -q\n",
    "print(torch_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "dies5CJuqmum"
   },
   "outputs": [],
   "source": [
    "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "!pip install torch-scatter -f $scatter_src -q\n",
    "!pip install torch-sparse -f $sparse_src -q\n",
    "!pip install torch-geometric -q\n",
    "!pip install ogb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uIk066oMxUJ",
    "outputId": "e1e6d757-9329-4f50-dd82-e0ebb2562660"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euGXjAPU1Ss4"
   },
   "source": [
    "### Setting up the dataset and tasks\n",
    "\n",
    "The below code sets up some hyperparameters which will be used in dataloading and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg2APSF7WvSq",
    "outputId": "fd588efb-a80a-4dac-895b-3530d8c81346"
   },
   "outputs": [],
   "source": [
    "# Model settings\n",
    "hidden_dim = 128 #@param {type: \"integer\"}\n",
    "num_layers = 4 #@param {type: \"integer\"}\n",
    "# Training settings\n",
    "learning_rate = 0.0001 #@param {type: \"number\"}\n",
    "num_epochs = 25 #@param {type: \"integer\"}\n",
    "\n",
    "# Dataloader settings\n",
    "batch_size = 32 #@param {type: \"integer\"}\n",
    "fan_out = 30 #@param {type: \"integer\", hint: \"Used in neighborhood sampling to sample a subgraph\"}\n",
    "dataloader_num_workers = 2 #@param {type: \"integer\"}\n",
    "\n",
    "print(f\"\"\"\n",
    "Running training with the following configuration:\n",
    "   hidden_dim: {hidden_dim}\n",
    "   num_layers: {num_layers}\n",
    "   learning_rate: {learning_rate}\n",
    "   num_epochs: {num_epochs}\n",
    "   batch_size: {batch_size}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNIrDcPdY9ot"
   },
   "source": [
    "### Setups\n",
    "\n",
    "Run the below block to create the dataset. If this is your first time loading the dataset, it will additionally prompt you to download files.\n",
    "\n",
    "**Note**: this block loads the dataset into RAM each time it is called! So calling this block multiple times will likely consume all of the notebook's RAM. Take caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjEjRXaMZLqZ",
    "outputId": "c6db83f1-00a8-4a28-9189-58789f0e01b6"
   },
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='./arxiv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bKdSKWebalo3",
    "outputId": "ec7c5d07-7c01-4114-f506-77404dd185f5"
   },
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split()\n",
    "# sample test set to speed up\n",
    "split_idx['test'] = split_idx['test']\n",
    "split_idx['valid'] = split_idx['valid']\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary of the OBGN Arxiv dataset:\n",
    "  Number of graphs: {len(dataset)}\n",
    "  Number of features: {dataset.num_features}\n",
    "  Number of classes: {dataset.num_classes}\n",
    "  Length of each split:\n",
    "    Training: {len(split_idx['train'])}\n",
    "    Validation: {len(split_idx['valid'])}\n",
    "    Test: {len(split_idx['test'])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUYQHiFYYloB"
   },
   "source": [
    "Now, let's create some dataloaders!\n",
    "\n",
    "todo: write something about neighbor loader. why do we need this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3loxjdsYkwv",
    "outputId": "9a81e1c7-3f4b-4a12-bfb3-9f3e9416705b"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "data = dataset[0]\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['train'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=dataloader_num_workers\n",
    ")\n",
    "val_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['valid'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=dataloader_num_workers,\n",
    ")\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    input_nodes=split_idx['test'],\n",
    "    num_neighbors=[fan_out] * num_layers,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0HmhMpxvuxO0",
    "outputId": "670b02fc-852c-4378-85ff-08a1bd071d52"
   },
   "outputs": [],
   "source": [
    "print(f\"Example batch:\")\n",
    "train_batch = next(iter(train_loader))\n",
    "print(train_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRim-oBTUVSe"
   },
   "source": [
    "### Training and Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RGBPUxkQ2R7H"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GraphSAGE\n",
    "\n",
    "input_dim = dataset.num_features\n",
    "\n",
    "def get_model():\n",
    "    class GraphSAGENodeClassification(torch.nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
    "            super(GraphSAGENodeClassification, self).__init__()\n",
    "            self.graph_sage = GraphSAGE(in_channels = input_dim, hidden_channels = hidden_dim, num_layers=num_layers)\n",
    "            self.cls_head = torch.nn.Sequential(\n",
    "                torch.nn.Dropout(0.1),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(hidden_dim, num_classes),\n",
    "            )\n",
    "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            h = self.graph_sage(x, edge_index)\n",
    "            return self.cls_head(h)\n",
    "\n",
    "    model = GraphSAGENodeClassification(input_dim, hidden_dim, num_layers, dataset.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQk7f7vrcPKR"
   },
   "source": [
    "Next, we define a simple training loop and evaluation function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KWZejP8NNFlG"
   },
   "outputs": [],
   "source": [
    "# training process\n",
    "def train_one_epoch(model,\n",
    "                    dataloader,\n",
    "                    optimizer,\n",
    "                    transform=None,\n",
    "                    filter_output_fn=None):\n",
    "    \"\"\"\n",
    "    Run one epoch of training on the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
    "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
    "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    # define stats\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if transform is not None:\n",
    "          batch = transform(batch)\n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch.x, batch.edge_index)\n",
    "        if filter_output_fn is not None:\n",
    "          logits = filter_output_fn(logits, batch)\n",
    "\n",
    "        # backward pass\n",
    "        num_labels = logits.shape[0]\n",
    "        labels = batch.y.squeeze(-1)\n",
    "\n",
    "        # select supervision nodes\n",
    "        labels = labels[:batch_size]\n",
    "        logits = logits[:batch_size]\n",
    "        num_examples += batch_size\n",
    "\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # log stats\n",
    "        total_loss += loss.item() * num_labels\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc\n",
    "\n",
    "# test process\n",
    "@torch.no_grad()\n",
    "def test(model,\n",
    "         dataloader,\n",
    "         transform=None,\n",
    "         filter_output_fn=None,\n",
    "         apply_transform=True):\n",
    "    \"\"\"\n",
    "    Calculate metrics for the model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
    "        apply_transform: whether to use the arguments transform and filter_output_fn.\n",
    "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
    "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # define states\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    num_examples = 0\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        # transform batch if needed\n",
    "        batch_size = batch.batch_size\n",
    "        batch = batch.to(device)\n",
    "        if apply_transform and (transform is not None):\n",
    "          batch = transform(batch)\n",
    "\n",
    "        # forward pass\n",
    "        logits = model(batch.x, batch.edge_index)\n",
    "        if apply_transform and (filter_output_fn is not None):\n",
    "          logits = filter_output_fn(logits, batch)\n",
    "\n",
    "        # compute loss\n",
    "        num_labels = logits.shape[0]\n",
    "        labels = batch.y.squeeze(-1)\n",
    "\n",
    "        # select supervision nodes\n",
    "        labels = labels[:batch_size]\n",
    "        logits = logits[:batch_size]\n",
    "        num_examples += batch_size\n",
    "\n",
    "        loss = model.loss_fn(logits, labels)\n",
    "        # log stats\n",
    "        total_loss += loss.item() * num_labels\n",
    "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
    "        \n",
    "\n",
    "    loss = total_loss / num_examples\n",
    "    acc = total_correct / num_examples\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r7qVZHKdIQi"
   },
   "source": [
    "To train and evaluate out model, call the below function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e41neWDGY_Mw"
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          num_epochs,\n",
    "          transform=None,\n",
    "          filter_output_fn=None,\n",
    "          apply_transform_at_test=True):\n",
    "    all_train_acc, all_val_acc, all_test_acc = [], [], []\n",
    "    best_val_ind, best_val_acc = 0, 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch: {epoch+1:02d}')\n",
    "\n",
    "        # training\n",
    "        train_loss, train_acc = train_one_epoch(model,\n",
    "                                                train_loader,\n",
    "                                                optimizer,\n",
    "                                                transform,\n",
    "                                                filter_output_fn)\n",
    "        val_loss, val_acc = test(model,\n",
    "                                 val_loader,\n",
    "                                 transform,\n",
    "                                 filter_output_fn=filter_output_fn,\n",
    "                                 apply_transform=apply_transform_at_test)\n",
    "        test_loss, test_acc = test(model,\n",
    "                                   test_loader,\n",
    "                                   transform,\n",
    "                                   filter_output_fn=filter_output_fn,\n",
    "                                   apply_transform=apply_transform_at_test)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_ind = epoch\n",
    "\n",
    "        print(f'Train {train_loss:.4f} ({100.0 * train_acc:.2f}%) | Val {val_loss:.4f} ({100.0 * val_acc:.2f}%) | Test {test_loss:.4f} ({100.0 * test_acc:.2f}%)')\n",
    "\n",
    "        all_train_acc.append(train_acc)\n",
    "        all_val_acc.append(val_acc)\n",
    "        all_test_acc.append(test_acc)\n",
    "\n",
    "    return {\n",
    "        'all_train_acc': np.array(all_train_acc),\n",
    "        'all_val_acc': np.array(all_val_acc),\n",
    "        'all_test_acc': np.array(all_test_acc),\n",
    "        'best_val_ind': best_val_ind,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7689hVEydrU7"
   },
   "source": [
    "To illustrate some example usage, let's run a baseline. This trains a GraphSAGE network with no graph structure augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJREZyVzPqIr",
    "outputId": "78657c28-589e-4515-e79a-7b1cc57b74bd"
   },
   "outputs": [],
   "source": [
    "model, optimizer = get_model()\n",
    "results = train(model, optimizer, num_epochs=num_epochs)\n",
    "\n",
    "best_bl_train_acc = results['all_train_acc'][results['best_val_ind']]\n",
    "best_bl_val_acc = results['all_val_acc'][results['best_val_ind']]\n",
    "best_bl_test_acc = results['all_test_acc'][results['best_val_ind']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFw4Vi5o1dPP"
   },
   "source": [
    "## Training Augmentation\n",
    "\n",
    "Beyond augmentations on the graph and its nodes as covered above, it is useful to dynamically augment batches during training, analogous to random cropping, blurring, and color shifting for images. To accomplish this, we refer to several methods in torch_geometric.utils. In this section, we introduce two training augmentation methods: “Mask Feature” and “Dropout Edge” These methods modify the sampled graph during training to dynamically perturb the input, preventing overfitting by discouraging the model from over-relying on specific features or edges. This approach leads to a more robust and generalizable model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktRl8GWF2R7I"
   },
   "source": [
    "### Dropout Edge\n",
    "The dropout_edge function randomly removes edges from the graph. It operates on the edge_index matrix and returns:\n",
    "* A modified edge_index with some edges dropped.\n",
    "* A binary tensor (edge_mask) indicating which edges were retained (True) or dropped (False).\n",
    "\n",
    "Key Arguments:\n",
    "\n",
    "* p (float, default=0.5): The probability of dropping an edge.\n",
    "* Force_undirected: When set to True setting ensures that the resulting edge_index remains undirected.\n",
    "\n",
    "The following code applied dropout edge with p $\\in$ [0.1, 0.2, 0.3, 0.4, 0.5]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import dropout_edge\n",
    "\n",
    "def dropout_edge_batch(batch, p):\n",
    "    batch.edge_index, removed_edge = dropout_edge(batch.edge_index, p=p)\n",
    "    return batch\n",
    "\n",
    "rows = []\n",
    "for prob in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking probability of {prob}\\n\")\n",
    "    transform = partial(dropout_edge_batch, p=prob)\n",
    "    result = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=num_epochs)\n",
    "    rows.append({\n",
    "        \"edge_prob\":prob,\n",
    "        \"test_acc\": result['all_test_acc'][result['best_val_ind']],\n",
    "        \"val_acc\": result['all_val_acc'][result['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"out/dropout_edge_result.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result: Higher dropout rate leads to generally better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('out/dropout_edge_result.csv', index_col=0)\n",
    "df.loc[5] = [0, 0.5418, 0.615]\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "ax = sns.lineplot(x='edge_prob', y='test_acc', data=df)\n",
    "\n",
    "ax.set(xlabel='Edge Dropout Probability', ylabel='Test Accuracy')\n",
    "ax.set_yticks([0.54, 0.55, 0.56], [\"54%\", \"55%\", \"56%\"])\n",
    "ax.set_ylim(0.538, 0.562)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Feature\n",
    "\n",
    "The mask_feature function randomly masks parts of node features. It takes the node feature matrix x as input and returns the modified features along with a mask indicating the positions of the masked features.\n",
    "\n",
    "Key arguments:\n",
    "* p: The probability of masking a feature.\n",
    "* fill_value: The value used to replace masked features (default: 0).\n",
    "* mode: The masking scheme.\n",
    "\n",
    "There are three masking modes:\n",
    "* col (default): Masks entire feature columns across all nodes.\n",
    "* row: Masks all features of selected nodes.\n",
    "* all: Masks individual features independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code applied feature masking with p $\\in$ [0.1, 0.2, 0.3, 0.4, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import mask_feature\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "\n",
    "def mask_feature_batch(batch, p, mode=\"all\"):\n",
    "    masked_x, feature_mask = mask_feature(batch.x, p = p, mode = mode)\n",
    "    batch.x = masked_x\n",
    "    return batch\n",
    "\n",
    "rows = []\n",
    "for masking_prob in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking probability of {masking_prob}\\n\")\n",
    "    transform = partial(mask_feature_batch, p=masking_prob)\n",
    "    results = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=25)\n",
    "    rows.append({\n",
    "        \"masking_prob\": masking_prob,\n",
    "        \"test_acc\": results['all_test_acc'][results['best_val_ind']],\n",
    "        \"val_acc\": results['all_val_acc'][results['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"/home/jerrychan/cs224_final_project/out/masking_prob_results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code applied feature masking with masking mode $\\in$ [\"col\", \"row\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for masking_mode in [\"col\", \"row\"]:\n",
    "    model, optimizer = get_model()\n",
    "    print(f\"Training with masking mode {masking_mode}\\n\")\n",
    "    transform = partial(mask_feature_batch, p=0.05, mode=masking_mode)\n",
    "    results = train(model, optimizer, transform=transform, apply_transform_at_test=False, num_epochs=25)\n",
    "    rows.append({\n",
    "        \"masking_mode\": masking_mode,\n",
    "        \"test_acc\": results['all_test_acc'][results['best_val_ind']],\n",
    "        \"val_acc\": results['all_val_acc'][results['best_val_ind']]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"/home/jerrychan/cs224_final_project/out/masking_mode_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
