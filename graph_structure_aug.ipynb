{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adelsuh/cs224_final_project/blob/main/graph_structure_aug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS6ESNtSpN-z"
      },
      "source": [
        "# CS224W Final Project: Tutorial on the Augmentation of Graphs in PyG using Structural Augmentations\n",
        "\n",
        "### Jerry Chan, Jihee Suh, John So"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation is a widely used technique that leverages existing data to further train a model, improving its performance and generalization. For structured data formats such as images, augmentation methods can be quite straightforward, including operations like cropping, resizing, rotating, and adding noise. These augmentations are useful for reducing overfitting to the training dataset and adding invariance to certain transformations, such as color shifts, different camera models, and even different camera poses.\n",
        "\n",
        "PyG provides support for dataset augmentations, which primarily inherit from the `torch_geometric.transforms` class. In this Colab, we will cover how augmentations can modify the graph structure to improve performance in inductive graph learning settings."
      ],
      "metadata": {
        "id": "e3x01tiCqdur"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPsmdGx7UJ4z"
      },
      "source": [
        "### Notebook setup: install PyG + torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pS2qTpqApIRS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch_version = str(torch.__version__)\n",
        "if \"2.4.0\" not in torch_version:\n",
        "  !pip install torch==2.4.0 -q\n",
        "print(torch_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dies5CJuqmum"
      },
      "outputs": [],
      "source": [
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src -q\n",
        "!pip install torch-sparse -f $sparse_src -q\n",
        "!pip install torch-geometric -q\n",
        "!pip install ogb -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uIk066oMxUJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euGXjAPU1Ss4"
      },
      "source": [
        "### Setting up the dataset and tasks\n",
        "\n",
        "The below code sets up some hyperparameters which will be used in dataloading and training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Eg2APSF7WvSq"
      },
      "outputs": [],
      "source": [
        "# Model settings\n",
        "hidden_dim = 128 #@param {type: \"integer\"}\n",
        "num_layers = 4 #@param {type: \"integer\"}\n",
        "# Training settings\n",
        "learning_rate = 0.0001 #@param {type: \"number\"}\n",
        "num_epochs = 25 #@param {type: \"integer\"}\n",
        "\n",
        "# Dataloader settings\n",
        "batch_size = 32 #@param {type: \"integer\"}\n",
        "fan_out = 30 #@param {type: \"integer\", hint: \"Used in neighborhood sampling to sample a subgraph\"}\n",
        "dataloader_num_workers = 2 #@param {type: \"integer\"}\n",
        "\n",
        "print(f\"\"\"\n",
        "Running training with the following configuration:\n",
        "   hidden_dim: {hidden_dim}\n",
        "   num_layers: {num_layers}\n",
        "   learning_rate: {learning_rate}\n",
        "   num_epochs: {num_epochs}\n",
        "   batch_size: {batch_size}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNIrDcPdY9ot"
      },
      "source": [
        "### Task: Node property prediction with GraphSAGE\n",
        "\n",
        "For this tutorial, we will consider the [ogbn-arxiv](https://ogb.stanford.edu/docs/nodeprop/) dataset from the Open Graph Benchmark (OGB). This dataset consists of ~170K nodes and ~1.2M directed edges. Each node represents an arXiv CS paper, and each edge represents the citation network between arXiv papers. The prediction task is to predict one of 40 labels for each node (i.e. CS.AI, CS.OS, etc.), given a 128-dimensional feature vector consisting of averaged language embeddings for the arXiv paper."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the below block to create the dataset. If this is your first time loading the dataset, it will additionally prompt you to download files.\n",
        "\n",
        "**Note**: this block loads the dataset into RAM each time it is called! So calling this block multiple times will likely consume all of the notebook's RAM. Take caution."
      ],
      "metadata": {
        "id": "TQcL9AQVqt36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjEjRXaMZLqZ"
      },
      "outputs": [],
      "source": [
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "dataset = PygNodePropPredDataset(name='ogbn-arxiv', root='./arxiv/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll approach this as an *inductive graph prediction* problem; we want to train one network across many graphs, such that when given new nodes in the graph, or entirely new graphs, our predictor can generalize. To aid with this, the `ogbn-arxiv` dataset provides dataset splits:\n",
        "\n",
        "- **train split**: CS papers published up until 2017\n",
        "- **validation split**: CS papers published during 2018\n",
        "- **test split**: CS papers published during or after 2019.\n",
        "\n",
        "We'll train on the subgraphs on the train split, then optimize parameters based on performance on the validation split. Lastly, we'll report the performance of methods on the test split."
      ],
      "metadata": {
        "id": "UdW5CelBqwK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKdSKWebalo3"
      },
      "outputs": [],
      "source": [
        "split_idx = dataset.get_idx_split()\n",
        "print(f\"\"\"\n",
        "Summary of the OBGN Arxiv dataset:\n",
        "  Number of graphs: {len(dataset)}\n",
        "  Number of features: {dataset.num_features}\n",
        "  Number of classes: {dataset.num_classes}\n",
        "  Length of each split:\n",
        "    Training: {len(split_idx['train'])}\n",
        "    Validation: {len(split_idx['valid'])}\n",
        "    Test: {len(split_idx['test'])}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUYQHiFYYloB"
      },
      "source": [
        "Now, let's create some dataloaders. Training GNNs on graphs with 100k+ node is computationally prohibitive, making full-batch training infeasible. To address this, we can use neighbor sampling, a technique designed for efficient mini-batch training on large graphs by sampling smaller subgraphs. In particular, PyG provides native functionality for this with the `NeighborLoader` class.\n",
        "\n",
        "The `num_neighbors` parameter controls how neighbors are sampled. It consists of a length $k$ list of integers, and performs $k$ sampling iterations. Starting from a sampled node, NeighborLoader samples `num_neighbors[i]` neighbors from nodes involved in the previous iteration. In our code, we sample $k$-hop neighborhoods, where $k$ is the depth of our GNN. This approach ensures scalability by iteratively constructing smaller, representative sub-graphs while preserving the graph structure necessary for effective learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3loxjdsYkwv"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "data = dataset[0]\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=split_idx['train'],\n",
        "    num_neighbors=[fan_out] * num_layers,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=dataloader_num_workers\n",
        ")\n",
        "val_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=split_idx['valid'],\n",
        "    num_neighbors=[fan_out] * num_layers,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=dataloader_num_workers,\n",
        ")\n",
        "test_loader = NeighborLoader(\n",
        "    data,\n",
        "    input_nodes=split_idx['test'],\n",
        "    num_neighbors=[fan_out] * num_layers,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HmhMpxvuxO0"
      },
      "outputs": [],
      "source": [
        "print(f\"Example batch:\")\n",
        "train_batch = next(iter(train_loader))\n",
        "print(train_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRim-oBTUVSe"
      },
      "source": [
        "### Training and Evaluation Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this dataset, let's now choose a GNN model. For the purposes of this tutorial, we will choose a simple yet powerful network. [GraphSage](https://arxiv.org/abs/1706.02216) (Hamilton 2017) leverages node features and neighborhood aggregation to learn deep features for nodes. This lends itself to a variety of prediction problems, including our node classification problem.\n",
        "\n",
        "PyG includes a native implementation of [GraphSAGE](https://pytorch-geometric.readthedocs.io/en/2.5.3/generated/torch_geometric.nn.models.GraphSAGE.html), which we will use to construct our predictor. The predictor consists of a GraphSAGE model to learn deep node embeddings. Then, we pass in a small MLP to output prediction logits for each node."
      ],
      "metadata": {
        "id": "QwWE9Jdeq2Ue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGBPUxkQ2R7H"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.models import GraphSAGE\n",
        "\n",
        "input_dim = dataset.num_features\n",
        "\n",
        "def get_model():\n",
        "    class GraphSAGENodeClassification(torch.nn.Module):\n",
        "        def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "            super(GraphSAGENodeClassification, self).__init__()\n",
        "            self.graph_sage = GraphSAGE(in_channels = input_dim, hidden_channels = hidden_dim, num_layers=num_layers)\n",
        "            self.cls_head = torch.nn.Sequential(\n",
        "                torch.nn.Dropout(0.1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(hidden_dim, num_classes),\n",
        "            )\n",
        "            self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        def forward(self, x, edge_index):\n",
        "            h = self.graph_sage(x, edge_index)\n",
        "            return self.cls_head(h)\n",
        "\n",
        "    model = GraphSAGENodeClassification(input_dim, hidden_dim, num_layers, dataset.num_classes)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    model.to(device)\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQk7f7vrcPKR"
      },
      "source": [
        "Next, we'll define some helper functions, `train_one_epoch` and `test` below. Notably, our functions will take in two arguments:\n",
        "- `transform`: whether to apply a unary transformation function to the data, such as removing nodes or adding features.\n",
        "- `filter_output_fn`: whether to apply a binary transformation function to the output. This is intended to handle any extra information introduced by the transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWZejP8NNFlG"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# training process\n",
        "def train_one_epoch(model,\n",
        "                    dataloader,\n",
        "                    optimizer,\n",
        "                    transform=None,\n",
        "                    filter_output_fn=None):\n",
        "    \"\"\"\n",
        "    Run one epoch of training on the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
        "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
        "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # define stats\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    num_examples = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "\n",
        "        # transform batch if needed\n",
        "        batch_size = batch.batch_size\n",
        "        batch = batch.to(device)\n",
        "        if transform is not None:\n",
        "          batch = transform(batch)\n",
        "        # forward pass\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch.x, batch.edge_index)\n",
        "        if filter_output_fn is not None:\n",
        "          logits = filter_output_fn(logits, batch)\n",
        "\n",
        "        # backward pass\n",
        "        labels = batch.y.squeeze(-1)\n",
        "\n",
        "        # use only the supervision nodes; the first batch_size nodes\n",
        "        logits = logits[:batch_size]\n",
        "        labels = labels[:batch_size]\n",
        "\n",
        "        loss = model.loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # log stats\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
        "        num_examples += batch_size\n",
        "\n",
        "    loss = total_loss / num_examples\n",
        "    acc = total_correct / num_examples\n",
        "    return loss, acc\n",
        "\n",
        "# test process\n",
        "@torch.no_grad()\n",
        "def test(model,\n",
        "         dataloader,\n",
        "         transform=None,\n",
        "         filter_output_fn=None,\n",
        "         apply_transform=True):\n",
        "    \"\"\"\n",
        "    Calculate metrics for the model on the given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        dataloader (torch.utils.data.DataLoader): The dataloader for the dataset.\n",
        "        apply_transform: whether to use the arguments transform and filter_output_fn.\n",
        "        transform: if specified and apply_transform is True, a transformation to apply to each batch\n",
        "        filter_output_fn: if specified and apply_transform is True, a transformation to apply to the output of each batch.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # define states\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    num_examples = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        # transform batch if needed\n",
        "        batch_size = batch.batch_size\n",
        "        batch = batch.to(device)\n",
        "        if apply_transform and (transform is not None):\n",
        "          batch = transform(batch)\n",
        "\n",
        "        # forward pass\n",
        "        logits = model(batch.x, batch.edge_index)\n",
        "        if apply_transform and (filter_output_fn is not None):\n",
        "          logits = filter_output_fn(logits, batch)\n",
        "\n",
        "        # use only the supervision nodes: the first batch_size nodes\n",
        "        labels = batch.y.squeeze(-1)\n",
        "\n",
        "        logits = logits[:batch_size]\n",
        "        labels = labels[:batch_size]\n",
        "        loss = model.loss_fn(logits, labels)\n",
        "        # log stats\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_correct += logits.argmax(dim=-1).eq(labels).sum().item()\n",
        "        num_examples += batch_size\n",
        "\n",
        "    loss = total_loss / num_examples\n",
        "    acc = total_correct / num_examples\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r7qVZHKdIQi"
      },
      "source": [
        "To train and evaluate the model, call the below `train` function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e41neWDGY_Mw"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          optimizer,\n",
        "          num_epochs,\n",
        "          transform=None,\n",
        "          filter_output_fn=None,\n",
        "          apply_transform_at_test=True):\n",
        "    all_train_acc, all_val_acc, all_test_acc = [], [], []\n",
        "    best_val_ind, best_val_acc = 0, 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch: {epoch+1:02d}')\n",
        "\n",
        "        # training\n",
        "        train_loss, train_acc = train_one_epoch(model,\n",
        "                                                train_loader,\n",
        "                                                optimizer,\n",
        "                                                transform,\n",
        "                                                filter_output_fn)\n",
        "        val_loss, val_acc = test(model,\n",
        "                                 val_loader,\n",
        "                                 transform,\n",
        "                                 filter_output_fn=filter_output_fn,\n",
        "                                 apply_transform=apply_transform_at_test)\n",
        "        test_loss, test_acc = test(model,\n",
        "                                   test_loader,\n",
        "                                   transform,\n",
        "                                   filter_output_fn=filter_output_fn,\n",
        "                                   apply_transform=apply_transform_at_test)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_val_ind = epoch\n",
        "\n",
        "        print(f'Train {train_loss:.4f} ({100.0 * train_acc:.2f}%) | Val {val_loss:.4f} ({100.0 * val_acc:.2f}%) | Test {test_loss:.4f} ({100.0 * test_acc:.2f}%)')\n",
        "\n",
        "        all_train_acc.append(train_acc)\n",
        "        all_val_acc.append(val_acc)\n",
        "        all_test_acc.append(test_acc)\n",
        "\n",
        "    return {\n",
        "        'all_train_acc': np.array(all_train_acc),\n",
        "        'all_val_acc': np.array(all_val_acc),\n",
        "        'all_test_acc': np.array(all_test_acc),\n",
        "        'best_val_ind': best_val_ind,\n",
        "        'model': model\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7689hVEydrU7"
      },
      "source": [
        "To illustrate some example usage, let's run a baseline. This trains a GraphSAGE network with no graph structure augmentations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer = get_model()\n",
        "results = train(model, optimizer, num_epochs=num_epochs)\n",
        "\n",
        "best_bl_train_acc = results['all_train_acc'][results['best_val_ind']]\n",
        "best_bl_val_acc = results['all_val_acc'][results['best_val_ind']]\n",
        "best_bl_test_acc = results['all_test_acc'][results['best_val_ind']]"
      ],
      "metadata": {
        "id": "HJREZyVzPqIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "if not os.path.exists(\"out\"):\n",
        "  os.mkdir(\"out\")\n",
        "\n",
        "rows = [{\n",
        "    \"edge_prob\": 0,\n",
        "    \"test_acc\": best_bl_test_acc,\n",
        "    \"val_acc\": best_bl_val_acc\n",
        "}]\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(\"out/bl_result.csv\")"
      ],
      "metadata": {
        "id": "dIY2tjtRrBQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the training curve."
      ],
      "metadata": {
        "id": "55V76hyurCjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Optional\n",
        "\n",
        "# Set a clean, modern aesthetic\n",
        "plt.style.use('seaborn-v0_8-pastel')\n",
        "sns.set_palette(\"deep\")\n",
        "\n",
        "def plot(x: Optional[np.ndarray] = None,\n",
        "         y: Dict[str, np.ndarray] = dict(),\n",
        "         xlabel: str = \"\",\n",
        "         ylabel: str = \"accuracy\"):\n",
        "\n",
        "  plt.figure(figsize=(6, 4), dpi=300)\n",
        "\n",
        "  for key, value in y.items():\n",
        "    if x is not None:\n",
        "      plt.plot(x, value, label=key)\n",
        "    else:\n",
        "      plt.plot(value, label=key)\n",
        "\n",
        "  plt.grid(True, linestyle='--', linewidth=0.5, color='grey', alpha=0.7)\n",
        "  plt.title('accuracy')\n",
        "  if x is not None:\n",
        "    plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "  plt.legend(frameon=True, fancybox=True, framealpha=0.7)\n",
        "  plt.tight_layout()\n",
        "  plt.gca().set_facecolor('none')\n",
        "  plt.gcf().patch.set_alpha(0.0)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Dpv163A4GlER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_plot = {\n",
        "    \"train\": results['all_train_acc'],\n",
        "    \"val\": results['all_val_acc'],\n",
        "}\n",
        "\n",
        "plot(y=to_plot, xlabel=\"epoch\")"
      ],
      "metadata": {
        "id": "wPbxf44hHYYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFw4Vi5o1dPP"
      },
      "source": [
        "## Graph Structure Augmentation\n",
        "\n",
        "Modifying the **struture** of the graph is also a powerful way to improve the performance of GNNs. The performance of GNNs is very much related to the structure of the graph.\n",
        "\n",
        "To better motivate graph structure augmentations, let’s first revisit the core idea of GNNs: message passing. At each layer, nodes aggregate information from their neighbors, gradually building a representation that reflects their local neighborhood structure. In theory, deeper networks should be able to capture broader relationships in the graph, integrating information from distant nodes.\n",
        "\n",
        "- **Over-smoothing**:    Recall that a GNN with $k$ layers aggregates information from each node's $k$-hop neighborhood. Thus, as the network deepens, node representations increasingly mix, and after many layers, nodes tend to converge to very similar representations. This “blending” means that the network struggles to distinguish between nodes, especially in large, densely connected graphs. In extreme cases, the output becomes almost uniform across all nodes, rendering the GNN ineffective for tasks like classification or clustering.\n",
        "\n",
        "- **Global relationships**:    While increasing the receptive field of each node by stacking more GNN layers might seem like a solution, it exacerbates the over-smoothing problem noted above, highlighting the trade-offs between depth and effective information propagation.\n",
        "\n",
        "Graph structure augmentations tackle these challenges head-on by altering the graph’s connectivity, introducing extra nodes and/or edges. to improve the flow of information across the graph, mitigate over-smoothing, and enable GNNs to better capture both local and global patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYXn7Cqm-Ff1"
      },
      "source": [
        "### Half Hop\n",
        "\n",
        "Half-Hop (introduced in [Azabou 2023](https://arxiv.org/abs/2308.09198)) enhances message passing in neural networks by inserting intermediate \"slow nodes\" between connected nodes in a graph. This approach mitigates over-smoothing and improves performance, especially in scenarios where neighboring nodes have different labels. The PyG documentation can be found [here](https://pytorch-geometric.readthedocs.io/en/stable/generated/torch_geometric.transforms.HalfHop.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's visualize what Half-Hop is doing. First, let's define an example graph:"
      ],
      "metadata": {
        "id": "xP4TTYRYsTIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "from pylab import show\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_nodes = 4\n",
        "simple_x = torch.arange(num_nodes)\n",
        "simple_x1 = simple_x + 1\n",
        "simple_x1[-1] = 0\n",
        "simple_edge_index = torch.stack([simple_x, simple_x1], dim=0)\n",
        "\n",
        "simple_x = (simple_x + 1) / 4\n",
        "simple_data = Data(x=simple_x, edge_index=simple_edge_index, num_nodes=num_nodes)\n",
        "simple_data, simple_data.x[0]\n",
        "\n",
        "G = to_networkx(simple_data)\n",
        "plt.figure(figsize=(3,3))\n",
        "pos = nx.spring_layout(G)\n",
        "node_color = [simple_data.x[node] for node in G.nodes()]\n",
        "nx.draw(G,\n",
        "        pos=pos,\n",
        "        cmap=plt.get_cmap('coolwarm'),\n",
        "        node_color=node_color,\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        with_labels=True)\n",
        "show()"
      ],
      "metadata": {
        "id": "F3qPJ_L4sV5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's apply the HalfHop transformation."
      ],
      "metadata": {
        "id": "7j8tv2409JJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import HalfHop\n",
        "hh = HalfHop(alpha=0.5, p=1.0)\n",
        "hh_data = hh(simple_data)\n",
        "\n",
        "G = to_networkx(hh_data)\n",
        "plt.figure(figsize=(3,3))\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "node_color = [hh_data.x[node] for node in G.nodes()]\n",
        "nx.draw(G,\n",
        "        pos=pos,\n",
        "        cmap=plt.get_cmap('coolwarm'),\n",
        "        node_color=node_color,\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        with_labels=True)\n",
        "show()"
      ],
      "metadata": {
        "id": "8Hxg0IpOsdhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we define the appropriate trasformations to train HalfHop. Note that we need to filter out the slow nodes which HalfHop creates, so that we are left only with the labels of the true graph's nodes."
      ],
      "metadata": {
        "id": "ix_Eq_159MiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rKVySq62R7H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from torch_geometric.transforms import HalfHop\n",
        "# blend features from src and dest equally (alpha=0.5).\n",
        "# add a virtual node to all edges (p=1)\n",
        "\n",
        "def hh_filter(outputs, batch):\n",
        "  # note that the labels (i.e. batch.y) do not need to be masked.\n",
        "  # dummy labels are not added\n",
        "  return outputs[~batch.slow_node_mask]\n",
        "\n",
        "probs = [0.1, 0.5, 1.0]\n",
        "best_hh_train_acc, best_hh_val_acc, best_hh_test_acc = [], [], []\n",
        "for hh_prob in probs:\n",
        "    hh_transform = HalfHop(alpha=0.5, p=hh_prob)\n",
        "    model, optimizer = get_model()\n",
        "    print(f\"Training with half-hop probability of {hh_prob}\\n\")\n",
        "    result = train(model,\n",
        "                   optimizer,\n",
        "                   num_epochs=num_epochs,\n",
        "                   transform=hh_transform,\n",
        "                   filter_output_fn=hh_filter,\n",
        "                   apply_transform_at_test=True)\n",
        "\n",
        "    best_ind = result['best_val_ind']\n",
        "    best_hh_train_acc.append(result['all_train_acc'][best_ind])\n",
        "    best_hh_val_acc.append(result['all_val_acc'][best_ind])\n",
        "    best_hh_test_acc.append(result['all_test_acc'][best_ind])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's plot the result. HalfHop outperforms the baseline marginally."
      ],
      "metadata": {
        "id": "4tlRZiewrOFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('out/hh_result.csv', index_col=0)\n",
        "df.loc[5] = [0, 0.5418, 0.615]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "ax = sns.barplot(\n",
        "    x='hh_prob',\n",
        "    y='test_acc',\n",
        "    data=df,\n",
        "    color='skyblue',\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i, fmt='%.4f', padding=3)\n",
        "\n",
        "ax.set(\n",
        "    xlabel='Half Hop Probability',\n",
        "    ylabel='Test Accuracy',\n",
        "    title='Test Accuracy vs Half Hop Probability'\n",
        ")\n",
        "\n",
        "# Format y-axis as percentages\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
        "\n",
        "# Adjust y limits to focus on the variation\n",
        "ax.set_ylim(0.53, 0.56)\n",
        "\n",
        "plt.tight_layout()\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "G2SV1BY1O6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJqO9Ikplx6g"
      },
      "source": [
        "### Virtual Node\n",
        "\n",
        "VirtualNode (introduced in [Gilmer 2017](https://arxiv.org/abs/1704.01212)) appends a virtual node to the given homogeneous graph that is connected to all other nodes. The virtual node serves as a global scratch space that each node both reads from and writes to in every step of message passing. This allows information to travel long distances during the propagation phase."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize what VirtualNode does. Here's an example graph:"
      ],
      "metadata": {
        "id": "nyrCv_4L9boV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "from pylab import show\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_nodes = 4\n",
        "simple_x = torch.arange(num_nodes)\n",
        "simple_x1 = simple_x + 1\n",
        "simple_x1[-1] = 0\n",
        "simple_edge_index = torch.stack([simple_x, simple_x1], dim=0)\n",
        "\n",
        "simple_x = (simple_x + 1) / 4\n",
        "simple_data = Data(x=simple_x, edge_index=simple_edge_index, num_nodes=num_nodes)\n",
        "\n",
        "G = to_networkx(simple_data)\n",
        "plt.figure(figsize=(3,3))\n",
        "pos = nx.spring_layout(G)\n",
        "node_color = [simple_data.x[node] for node in G.nodes()]\n",
        "nx.draw(G,\n",
        "        pos=pos,\n",
        "        cmap=plt.get_cmap('coolwarm'),\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        nodelist=G.nodes(),\n",
        "        node_color=node_color,\n",
        "        with_labels=True)\n",
        "show()"
      ],
      "metadata": {
        "id": "zILNR_I06xzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the virtual node. It is connected bi-directionally to all nodes, and has an initial feature of 0."
      ],
      "metadata": {
        "id": "bCPzx7Y99ilj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import VirtualNode\n",
        "vn = VirtualNode()\n",
        "vn_data = vn(simple_data)\n",
        "\n",
        "G = to_networkx(vn_data)\n",
        "plt.figure(figsize=(3,3))\n",
        "pos = nx.spring_layout(G)\n",
        "\n",
        "node_color = [vn_data.x[node] for node in G.nodes()]\n",
        "nx.draw(G,\n",
        "        pos=pos,\n",
        "        cmap=plt.get_cmap('coolwarm'),\n",
        "        nodelist=G.nodes(),\n",
        "        vmin=0,\n",
        "        vmax=1,\n",
        "        node_color=node_color,\n",
        "        with_labels=True)\n",
        "show()"
      ],
      "metadata": {
        "id": "fUNuwxIg6zk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's run training! Note that the virtual node transformation adds an extra dummy label and dummy node to both batch.x and batch.labels. It is fine to leave these in."
      ],
      "metadata": {
        "id": "lmYTvgRMCwE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHfjmxWjlx6h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from torch_geometric.transforms import VirtualNode\n",
        "\n",
        "def vn_filter(outputs, batch):\n",
        "  # virtual node adds a dummy label (0), and a dummy node (0)\n",
        "  return outputs\n",
        "\n",
        "vn_transform = VirtualNode()\n",
        "model, optimizer = get_model()\n",
        "print(f\"Training with virtual node\\n\")\n",
        "result = train(model,\n",
        "                optimizer,\n",
        "                num_epochs=num_epochs,\n",
        "                transform=vn_transform,\n",
        "                filter_output_fn=vn_filter,\n",
        "                apply_transform_at_test=True)\n",
        "\n",
        "best_ind = result['best_val_ind']\n",
        "best_vn_train_acc = result['all_train_acc'][best_ind]\n",
        "best_vn_val_acc = result['all_val_acc'][best_ind]\n",
        "best_vn_test_acc = result['all_test_acc'][best_ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBLiHgQQ2R7I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create DataFrame with results\n",
        "df = pd.DataFrame({\n",
        "    'model': ['baseline', 'virtual node'],\n",
        "    'test_acc': [best_bl_test_acc, best_vn_test_acc]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "ax = sns.barplot(\n",
        "    x='model',\n",
        "    y='test_acc',\n",
        "    data=df,\n",
        "    color='skyblue',\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i, fmt='%.4f', padding=3)\n",
        "\n",
        "ax.set(\n",
        "    xlabel='Model Type',\n",
        "    ylabel='Test Accuracy',\n",
        "    title='Test Accuracy: Baseline vs Virtual Node'\n",
        ")\n",
        "\n",
        "# Format y-axis as percentages\n",
        "ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
        "\n",
        "# Adjust y limits to focus on variation\n",
        "ymin = min(df['test_acc']) * 0.95\n",
        "ymax = max(df['test_acc']) * 1.05\n",
        "ax.set_ylim(ymin, ymax)\n",
        "\n",
        "plt.tight_layout()\n",
        "sns.despine()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}